{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (Reinforcement Learning)\n",
    "\n",
    "Deep Q-Learning is a combination of Q-learning and Deep Neural Networks used for decision-making in reinforcement learning. It is commonly used in game AI, robotics, and autonomous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Reinforcement Learning?\n",
    "\n",
    " In Reinforcement Learning (RL), an agent interacts with an environment and learns by receiving rewards or penalties based on its actions.\n",
    "\n",
    "The goal is to maximize cumulative rewards over time.\n",
    "\n",
    "Key Components of RL\n",
    "\n",
    "Agent : The decision-making entity.\n",
    "\n",
    "Environment : The world the agent interacts with.\n",
    "\n",
    "State (S) : A representation of the environment at a given time.\n",
    "\n",
    "Action (A) : The possible moves the agent can take.\n",
    "\n",
    "Reward (R) : Feedback for actions taken.\n",
    "\n",
    " Example: Training an AI to play a game like Flappy Bird or Atari."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning (DQN)\n",
    "\n",
    "Instead of using a Q-table, we use a Deep Neural Network (DNN) to approximate Q-values.\n",
    "\n",
    " Steps in Deep Q-Learning:\n",
    "\n",
    "Use a Neural Network to predict Q-values.\n",
    "\n",
    "Store past experiences in a Replay Buffer to prevent overfitting.\n",
    "\n",
    "Train the network using Mean Squared Error (MSE) loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Deep Q-Learning in Python\n",
    "\n",
    "We will train an AI to play CartPole using Deep Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: \n",
    "\n",
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym tensorflow numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: \n",
    "\n",
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "\n",
    " Create the Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_size, action_size):\n",
    "    model = Sequential([\n",
    "        Dense(24, input_dim=state_size, activation=\"relu\"),\n",
    "        Dense(24, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")  # Output Q-values\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes a state and predicts Q-values for each action.\n",
    "\n",
    "Loss Function: Mean Squared Error (MSE) between predicted Q-values and target Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "\n",
    " Create the Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)  # Experience Replay Buffer\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration-exploitation balance\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = build_model(state_size, action_size)\n",
    "    \n",
    "    # Store experience\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # Choose action (Exploration vs. Exploitation)\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:  # Random action (exploration)\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)  # Predict Q-values\n",
    "        return np.argmax(q_values[0])  # Choose best action (exploitation)\n",
    "    \n",
    "    # Train the model using experiences from memory\n",
    "    def replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:  # Bellman Equation\n",
    "                target += self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            \n",
    "            target_q_values = self.model.predict(state)\n",
    "            target_q_values[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_q_values, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:  # Decay epsilon\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Train the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")  # Create environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    for time in range(500):  # Limit steps per episode\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)  # Store experience\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {episode}/{episodes} - Score: {total_reward} - Epsilon: {agent.epsilon:.2f}\")\n",
    "            break\n",
    "            \n",
    "    agent.replay(batch_size)  # Train on experiences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Evaluate the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    action = np.argmax(agent.model.predict(state)[0])  # Use best action\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
