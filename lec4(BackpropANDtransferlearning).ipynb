{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation in Modern Neural Networks\n",
    "\n",
    "Techniques to Improve Learning\n",
    "\n",
    "Transfer Learning (Using pre-trained models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Backpropagation\n",
    "\n",
    "Backpropagation is how neural networks learn by adjusting weights using gradient descent.\n",
    "\n",
    "\n",
    "Key Steps in Backpropagation\n",
    "\n",
    "Forward Pass: Compute predictions.\n",
    "\n",
    "Loss Calculation: Measure error (e.g., categorical crossentropy).\n",
    "\n",
    "Backward Pass: Compute gradients using the chain rule.\n",
    "\n",
    "Weight Update: Use gradients to adjust weights (via SGD, Adam, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Backpropagation Techniques\n",
    "\n",
    " Adaptive Learning Rate Optimization\n",
    " \n",
    "Instead of a fixed learning rate, optimizers like Adam adjust learning rates dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping\n",
    "\n",
    "Prevents exploding gradients by capping gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization\n",
    "\n",
    "Normalizes activations between layers to stabilize learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Scheduling\n",
    "\n",
    "Reduces learning rate over time to refine training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01, decay_steps=1000, decay_rate=0.96)\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning (Using Pre-trained Models)\n",
    "\n",
    "Transfer learning allows us to use a pre-trained model (like VGG16, ResNet) and fine-tune it for a new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Using VGG16 for Cats vs. Dogs Classification\n",
    "\n",
    "Instead of training from scratch, we use VGG16 trained on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load a Pre-trained Model (Without Top Layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load pre-trained VGG16 model (without top layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers (don't train them)\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Add Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top of VGG16\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  # Binary classification (Cat vs. Dog)\n",
    "\n",
    "# Define final model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compile & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train with new dataset\n",
    "model.fit(train_data, epochs=5, validation_data=val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "**Technique**\t           **Purpose**\t                   **Benefit**\n",
    "\n",
    "**Gradient Clipping**\t     Prevents exploding gradients\tStabilizes deep networks\n",
    "\n",
    "**Batch Normalization**      Normalizes activations\t        Faster training\n",
    "\n",
    "**Learning Rate Scheduling**\tAdjusts learning rate dynamically\t Higher accuracy\n",
    "\n",
    "**Transfer Learning**\tUses pre-trained models\t     Saves time, improves performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
