{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization in Deep Learning\n",
    "\n",
    "Regularization helps prevent overfitting by reducing the complexity of the model. Two common techniques are L1 & L2 regularization and Dropout.\n",
    "\n",
    "\n",
    "L1 & L2 Regularization (Lasso & Ridge)\n",
    "\n",
    "L1 (Lasso): Shrinks some weights to exactly zero, promoting sparsity.\n",
    "\n",
    "L2 (Ridge): Shrinks weights but does not set them to zero, reducing their magnitude smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing L2 Regularization in a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# Create a neural network with L2 regularization\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  # Input layer\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
    "    layers.Dense(10, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect: \n",
    "\n",
    "L2 regularization reduces large weight values, making the model more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout Regularization\n",
    "\n",
    "Dropout randomly drops neurons during training, forcing the network to learn redundant and generalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Dropout in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer with 50% probability\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect: Dropout prevents over-reliance on specific neurons, improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters in Deep Learning\n",
    "\n",
    "Hyperparameters control the learning process but are not learned by the model. Some key hyperparameters are:\n",
    "\n",
    "Learning Rate (lr)\n",
    "Batch Size\n",
    "Number of Epochs\n",
    "Number of Layers & Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Learning Rate\n",
    "\n",
    "A small learning rate can slow down learning, while a large one may cause instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a custom learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Effect: Proper tuning of the learning rate improves convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Sets & Cross-Validation\n",
    "\n",
    "Validation Set\n",
    "\n",
    "A validation set helps in tuning hyperparameters and avoiding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training, validation, and test sets\n",
    "(x_train, x_valid) = x_train[:50000], x_train[50000:]\n",
    "(y_train, y_valid) = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# Train model with validation set\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross-Validation\n",
    "\n",
    "Instead of using a single validation set, K-Fold Cross-Validation splits data into K subsets, training on K-1 and validating on the remaining fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model.fit(x_train_fold, y_train_fold, epochs=5, validation_data=(x_val_fold, y_val_fold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect: Ensures the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised vs. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning\n",
    "\n",
    "Training data has input-output pairs.\n",
    "\n",
    "Example: Image classification, sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training a supervised neural network on labeled images\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning\n",
    "\n",
    "No labels; model finds patterns.\n",
    "\n",
    "Example: Clustering, anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: K-Means Clustering on Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Flatten images\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)  # 10 clusters for digits 0-9\n",
    "kmeans.fit(x_train_flat)\n",
    "\n",
    "# Get cluster assignments\n",
    "labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Key Concepts\n",
    "\n",
    "**Concept**\t              **Purpose**\t                    **Code Example**\n",
    "\n",
    "**L1/L2 Regularization** \tPrevent overfitting\t         regularizers.l2(0.01)\n",
    "\n",
    "**Dropout**\t          Improve generalization\t          layers.Dropout(0.5)\n",
    "\n",
    "**Learning Rate**\t   Control step size\t     optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "**Validation Set**\t   Tune hyperparameters\t      validation_data=(x_valid, y_valid)\n",
    "\n",
    "**Cross-Validation**\t  Robust model training\t            KFold(n_splits=5)\n",
    "\n",
    "**Supervised Learning**\t    Labeled data\t             model.fit(x_train, y_train)\n",
    "\n",
    "**Unsupervised Learning**\t  Discover patterns\t      KMeans(n_clusters=10).fit    (x_train_flat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
