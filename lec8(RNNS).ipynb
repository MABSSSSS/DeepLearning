{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) & Long Short-Term Memory (LSTMs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data such as text, speech, and time-series data. \n",
    "\n",
    "Unlike traditional neural networks, RNNs have a memory that allows them to learn from previous inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Do We Need RNNs?\n",
    "\n",
    "Traditional feedforward networks cannot handle sequential dependencies because they treat all inputs independently.\n",
    "\n",
    " Example:\n",
    "\n",
    "\"The cat is sitting on the ...\" ‚Üí The next word is likely \"mat\" (context matters).\n",
    "\n",
    "A standard neural network would not remember previous words to predict the next word.\n",
    "\n",
    " Solution: Use RNNs to maintain context across time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Architecture\n",
    "\n",
    "Each neuron in an RNN takes input from the previous time step.\n",
    "\n",
    "\n",
    "ht=tanh(W xx t+W hh t‚àí1+b)\n",
    "\n",
    "Where:\n",
    "\n",
    "x t= Input at time ùë°\n",
    "\n",
    "ht= Hidden state at time t (stores memory).\n",
    "\n",
    "W x,W h = Learnable weights.\n",
    "\n",
    "b = Bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Simple RNN (Text Prediction)\n",
    "\n",
    "We'll build a simple RNN that predicts the next word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare Data (Text to Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "text = [\"hello how are you\", \"I am fine thank you\", \"how about you\"]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# Create input (X) and output (Y) pairs\n",
    "X = np.array([seq[:-1] for seq in sequences])  # Input sequence\n",
    "y = np.array([seq[-1] for seq in sequences])   # Next word\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=10),  # Word embedding layer\n",
    "    SimpleRNN(50, activation=\"tanh\"),  # RNN layer with 50 units\n",
    "    Dense(vocab_size, activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTMs)\n",
    "\n",
    "Why Do We Need LSTMs?\n",
    "\n",
    "Problem with Vanilla RNNs:\n",
    "\n",
    "They suffer from vanishing gradients, meaning they forget long-term dependencies.\n",
    "\n",
    " Solution: LSTMs introduce memory cells that store long-term information.\n",
    "\n",
    "LSTM Cell Structure\n",
    "\n",
    "LSTMs have gates that regulate memory flow:\n",
    "\n",
    "Forget Gate üö™: Decides what to remove.\n",
    "\n",
    "Input Gate üìù: Decides what to store.\n",
    "\n",
    "Output Gate üì§: Decides what to output.\n",
    "\n",
    "\n",
    "ht=ot√ótanh(Ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing LSTM for Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=10),\n",
    "    LSTM(50, activation=\"tanh\"),  # Replace SimpleRNN with LSTM\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) - An Alternative to LSTM\n",
    "\n",
    "GRUs work like LSTMs but have fewer parameters, making them faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=10),\n",
    "    GRU(50, activation=\"tanh\"),  # Use GRU instead of LSTM\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: RNN vs. LSTM vs. GRU\n",
    "\n",
    "**Model**\t**Strength**\t**Weakness**\t**Best For**\n",
    "\n",
    "**RNN**\t   Works for short sequences\tVanishing gradient problem\tSimple text data\n",
    "\n",
    "**LSTM**\tHandles long sequences\tMore parameters, slower\tSentiment analysis, speech recognition\n",
    "\n",
    "**GRU**\tFaster than LSTM\tLess control over memory\tChatbots, real-time task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
