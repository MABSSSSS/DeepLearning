{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Compression: Pruning & Quantization\n",
    "\n",
    "Deep compression techniques reduce the size of deep neural networks while maintaining accuracy. \n",
    "\n",
    "This makes models faster, memory-efficient, and deployable on edge devices (like mobile phones, IoT devices).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Compression Needed?\n",
    "\n",
    "Deep learning models have millions of parameters, requiring high computational power.\n",
    "\n",
    "Problems with Large Models:\n",
    "\n",
    "Slow inference time.\n",
    "\n",
    "High memory usage.\n",
    "\n",
    "Difficult to deploy on edge devices.\n",
    "\n",
    "Solution: Use pruning & quantization to compress models while keeping accuracy high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning (Removing Unnecessary Weights)\n",
    "\n",
    "Pruning removes low-importance connections from a neural network, reducing the number of parameters.\n",
    "\n",
    " Types of Pruning:\n",
    "\n",
    "Weight Pruning: Removes small weight values.\n",
    "\n",
    "Neuron Pruning: Removes entire neurons that contribute less.\n",
    "\n",
    "Structured Pruning: Removes entire filters in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    " Train a Simple Model\n",
    "\n",
    "We first train a fully connected neural network on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    " Apply Weight Pruning\n",
    "\n",
    "\n",
    "We use TensorFlow Model Optimization Toolkit to prune small weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Define pruning parameters\n",
    "pruning_params = tfmot.sparsity.keras.PruningSchedule(\n",
    "    tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.2, final_sparsity=0.8, begin_step=2000, end_step=4000)\n",
    ")\n",
    "\n",
    "# Apply pruning to the model\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_params)\n",
    "\n",
    "# Compile pruned model\n",
    "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train pruned model\n",
    "pruned_model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "\n",
    "Convert Pruned Model to Normal Model\n",
    "\n",
    "After pruning, we strip unnecessary parameters to save storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip pruning\n",
    "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Save model\n",
    "pruned_model.save(\"pruned_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization (Reducing Precision of Weights)\n",
    "\n",
    "Quantization reduces the precision of model weights, typically from 32-bit floats to 8-bit integers.\n",
    "\n",
    "This greatly reduces model size and speeds up inference.\n",
    "\n",
    " Types of Quantization:\n",
    "\n",
    "Post-Training Quantization (Convert after training)\n",
    "\n",
    "Quantization-Aware Training (Train with lower precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    " Apply Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model(\"pruned_model.h5\")\n",
    "\n",
    "# Convert model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save compressed model\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
